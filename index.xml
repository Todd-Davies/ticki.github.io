<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ticki</title>
    <link>/</link>
    <description>Recent content on Ticki</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Sep 2016 22:50:08 +0200</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Rust&#39;s `std::collections` is absolutely horrible</title>
      <link>/blog/horrible/</link>
      <pubDate>Mon, 12 Sep 2016 22:50:08 +0200</pubDate>
      
      <guid>/blog/horrible/</guid>
      <description>

&lt;p&gt;Rust is by far my favorite language, and I am very familiar with it, but there is one aspect that annoys me a lot: &lt;code&gt;std::collections&lt;/code&gt;, a part of the opt-out standard library.&lt;/p&gt;

&lt;p&gt;This post (with a rather edgy title) will go through the short-fallings of the API and implementation of &lt;code&gt;std::collections&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&#34;what-it-contains&#34;&gt;What it contains&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;std::collections&lt;/code&gt; has a rather small set of collections (which is a legitimate choice to make to preserve minimality), the catch being that it&amp;rsquo;s an odd choice of collections:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;B-tree based map and set.&lt;/li&gt;
&lt;li&gt;Binary heap.&lt;/li&gt;
&lt;li&gt;Hash table and set.&lt;/li&gt;
&lt;li&gt;Doubly-linked list.&lt;/li&gt;
&lt;li&gt;Ring buffer.&lt;/li&gt;
&lt;li&gt;Random-acess vectors (strictly speaking not in &lt;code&gt;std::collections&lt;/code&gt; but instead in &lt;code&gt;std::vec&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That seems fine, doesn&amp;rsquo;t it? No, it doesn&amp;rsquo;t. If you consider what it lacks of these are very weird choices of structures.&lt;/p&gt;

&lt;p&gt;Take binary heap. It is incredibly useful at times, but is it really fit for a standard library with focus on being minimal? Let&amp;rsquo;s look at the statistics:&lt;/p&gt;

&lt;p&gt;444 examples of usage of this structure (in Rust) on GitHub. Now, we obviously cannot be sure that this sample is representative, but it should give a pretty good insight on the usage.&lt;/p&gt;

&lt;p&gt;Looking through these, approximately 50 of these are tests of &lt;code&gt;BinaryHeap&lt;/code&gt; itself. Another 50 are reimplementations of it. Around 100 of them are duplicates of other codes (e.g. downloaded libraries). This leaves us with around 250 usages, and that&amp;rsquo;s only slightly more than the incredibly useful &lt;code&gt;VecMap&lt;/code&gt;, which isn&amp;rsquo;t even in the standard library!&lt;/p&gt;

&lt;p&gt;If minimalism really is a goal (which I am going to criticise in a minute), it seems rather weird to have a collection which is barely used more than a non-libstd collection.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s move on to doubly-linked list. Searching on GitHub gives you 534 results. I was unable to find &lt;em&gt;a single place where it was used in a manner that could not be replaced by singly-linked lists&lt;/em&gt;. Chances are that there are some, but they&amp;rsquo;re incredibly rare, and it is odd given that there are no singly-linked list structures in the standard libraries.&lt;/p&gt;

&lt;h1 id=&#34;what-it-doesn-t-contain&#34;&gt;What it doesn&amp;rsquo;t contain&lt;/h1&gt;

&lt;h2 id=&#34;concurrent-data-structures&#34;&gt;Concurrent data structures&lt;/h2&gt;

&lt;p&gt;The whole standard library contains exactly two concurrent data structures (note that data structures are different from containers), namely the MPSC-queues (the blocking queue with a limited buffer and the non-blocking with an unlimited buffer). These are used for cross-thread message passing and the alike.&lt;/p&gt;

&lt;p&gt;But where are all the other concurrent primitives?&lt;/p&gt;

&lt;p&gt;People tend to wrap their structure in &lt;code&gt;Mutex&lt;/code&gt;, like &lt;code&gt;Mutex&amp;lt;HashMap&amp;lt;...&amp;gt;&amp;gt;&lt;/code&gt;, but that is often an order of magnitude slower than a concurrent hash table.&lt;/p&gt;

&lt;p&gt;Then there&amp;rsquo;s the multithreaded push/pop stacks (as opposed to the queue/unqueue lists), and so on.&lt;/p&gt;

&lt;p&gt;There are quite a few implementations of structures as the ones described above, but they are more often than not poorly implemented. The leading library (which has a pretty good implementation quality) is &lt;a href=&#34;https://github.com/aturon/crossbeam&#34;&gt;crossbeam&lt;/a&gt;, but unfortunately it only implements a very limited set of synchronization primitives (no maps, no tables, no skip lists, etc.).&lt;/p&gt;

&lt;h2 id=&#34;singly-linked-lists&#34;&gt;Singly linked lists&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve already mentioned this, but singly linked list are often useful.&lt;/p&gt;

&lt;h2 id=&#34;priority-queues&#34;&gt;Priority queues&lt;/h2&gt;

&lt;p&gt;Priority queues is the structure everyone ask about and looks for, but no one can name it (here&amp;rsquo;s an exercise: go on Google and try to vaguely describe this structure, you will for sure find at least one thread asking for exactly that description, and often no one is able to answer the question or misguidedly proposes binary heaps instead).&lt;/p&gt;

&lt;p&gt;Say you have an ordered list of elements, each of which has a priority. Now, you want to be able to retrieve the element with the highest or the lowest priority, with a reasonable performance. Note that mere heaps are not sufficient, since they are not arbitrarily ordered.&lt;/p&gt;

&lt;p&gt;Priority queues are used everywhere from cache level regulation to efficient scheduling, and are in my opinion one of the most useful data structures of all.&lt;/p&gt;

&lt;p&gt;It is hard to find out exactly how much it is used in the Rust community, given its many names and reimplementations. Only 83 occurrences of the name &amp;ldquo;PriorityQueue&amp;rdquo; in Rust code can be found on GitHub, but I suspect the real number to be much higher.&lt;/p&gt;

&lt;h2 id=&#34;treaps&#34;&gt;Treaps&lt;/h2&gt;

&lt;p&gt;Treaps are generally faster than other self-balanced trees (on the average), but the really killer feature is the bulk operations. These are highly efficient algorithms for union, intersections, and set differences.&lt;/p&gt;

&lt;p&gt;When the programmer is manipulating sets like this (union, intersections, and so on) and iterators aren&amp;rsquo;t sufficient (i.e., it is for storage, not iteration), these can be incredibly useful as a high-performance data structure.&lt;/p&gt;

&lt;h2 id=&#34;skip-lists&#34;&gt;Skip lists&lt;/h2&gt;

&lt;p&gt;Skip lists are more niche than the structures described above, but they have excellent performance characteristics.&lt;/p&gt;

&lt;p&gt;Skip lists are conceptually similar to N-ary trees, but in the representation of a list. They&amp;rsquo;re a probabilistic data structure, which holds a list and some number of sublists such that the &lt;em&gt;n&lt;/em&gt;&amp;lsquo;th sublist is a sublist of the &lt;em&gt;n - 1&lt;/em&gt;&amp;lsquo;th sublist. Search can be visualized as binary search by observing how two paths can be taken: A) go to the next sublist B) follow the link.&lt;/p&gt;

&lt;p&gt;The reason a good implementation outperforms a good implementation of classical binary search trees has to do with two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;On average, 50% of the links followed under a search are cache local, whereas B-trees, for example, are around 0% cache local.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;No tree rotations or equivalent operations are needed during insertion.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The downside is that they are not as rigidly balanced, making them slower in some cases.&lt;/p&gt;

&lt;h2 id=&#34;self-balancing-trees&#34;&gt;Self-balancing trees&lt;/h2&gt;

&lt;p&gt;As mentioned, Rust&amp;rsquo;s standard library already has an excellent implementation of B-trees, a popular form of self-balancing trees.&lt;/p&gt;

&lt;p&gt;The other popular self-balancing trees are good candidates as well (AVL and LLRB/Red-black). While they do essentially the same, they can have very different performance characteristics, and switching can influence the program&amp;rsquo;s performance vastly.&lt;/p&gt;

&lt;p&gt;Having a diverse set of such structures can be good, especially if the documentation details which one to use based on your use case.&lt;/p&gt;

&lt;h2 id=&#34;slobs-aka-pointer-lists-memory-pools-typed-arenas-etc&#34;&gt;SLOBs (aka. pointer lists, memory pools, typed arenas, etc.)&lt;/h2&gt;

&lt;p&gt;This is a very simple, and yet very powerful, data structure. In fact, they are nothing but a glorified singly linked list of pointers to some type. The dealbreaker is the fact that it requires no storage aside from the data it holds.&lt;/p&gt;

&lt;p&gt;That is, no allocation is needed to push and pop pointers from this list. This is possible by letting the data which is inactive hold the list itself.&lt;/p&gt;

&lt;p&gt;So what&amp;rsquo;s the big deal here? It turns out to be extremely useful for region-based memory management. If you have a lot of allocations of the same type, it is often multiple orders of magnitude faster than allocating each of them seperately, and what&amp;rsquo;s even cooler is the data locality it provides: Since it is based on one big contagious segment broken down into pieces, it will only cover a few pages, and consequently it is cache efficient (this fact will be abused in a minute).&lt;/p&gt;

&lt;h1 id=&#34;we-can-just-leave-it-to-other-libraries&#34;&gt;&amp;ldquo;We can just leave it to other libraries&amp;rdquo;&lt;/h1&gt;

&lt;p&gt;A common talking point is that we can simply outsource it to external libraries. Unfortunately, they cannot provide an essential property of the standard library: standardization. Standard libraries serves for making sure t
he ecosystem is uniform. If something is crucial for keeping the ecosystem together, it deserves a place in the standard library. These are severely underused due to the stigma around adding new dependencies.&lt;/p&gt;

&lt;p&gt;Standardization is absolutely crucial for adaptation.&lt;/p&gt;

&lt;h1 id=&#34;criticizing-the-structures-it-do-have&#34;&gt;Criticizing the structures it do have&lt;/h1&gt;

&lt;h2 id=&#34;hashmap&#34;&gt;&lt;code&gt;HashMap&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Rust&amp;rsquo;s hash table implementation is perhaps the worst part, not because it is particularly bad in context, but because it is a very performance-critical component, and yet has serious flaws.&lt;/p&gt;

&lt;p&gt;A quick overview of the Rust &lt;code&gt;HashMap&lt;/code&gt;/&lt;code&gt;HashSet&lt;/code&gt; implementation is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Open addressing (Robin Hood hashing)&lt;/li&gt;
&lt;li&gt;90.9% load factor before reallocation&lt;/li&gt;
&lt;li&gt;Defaults to Sip-hasher (cryptographic hash function)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s just go through these one-by-one and see what&amp;rsquo;s wrong:&lt;/p&gt;

&lt;h3 id=&#34;robin-hood-hashing&#34;&gt;Robin Hood hashing&lt;/h3&gt;

&lt;p&gt;Robin Hood hashing is a double-hashing variant quite, in which you rehash until the slot is free. Robin Hood hashing improves plain double-hashing by making sure the slots occupants are ordered by the probe length.&lt;/p&gt;

&lt;p&gt;So what&amp;rsquo;s the problem here? Well, the cache efficiency is not exactly ideal, but we get freedom from clustering in return.&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;opposite&amp;rdquo; approach is linear probing (where you add some constant - often 1 - to the slot number until it is free), which has the opposite nature: Cache efficiency is really good, but it is very sensitive to clustering.&lt;/p&gt;

&lt;p&gt;A reasonable alternative which takes the best of each of these solutions is quadratic probing, which simply uses a quadratic polynomial to jump between slots (or, in analogy to the one given above, the constant in question increases linearly).&lt;/p&gt;

&lt;p&gt;In most scenarios (especially for large tables), quadratic probing has fewer cache misses, due to better data locality.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s no reason to have strong opinions on this subject. The difference is rather small, but interesting nonetheless.&lt;/p&gt;

&lt;h3 id=&#34;a-high-reallocation-threshold&#34;&gt;A high reallocation threshold&lt;/h3&gt;

&lt;p&gt;This mostly comes down to a trade-off between between memory and CPU. If you think about it, 1:9 empty slots is a pretty dense table with an average probe length of 6 rehashes. Potentially (for very large tables) that can lead to 6 cache misses for just a single lookup.&lt;/p&gt;

&lt;p&gt;The advantage is that it is relatively memory efficient, but that should really only be a concern for really big tables. For most tables, this is way too high, and it trades CPU cycles for memory (which is almost unimportant these days).&lt;/p&gt;

&lt;p&gt;I personally think that having a constant factor is a bad idea. I think it should be some function of the number of elements in the table, such that the factor is lower for small tables (where memory isn&amp;rsquo;t a concern). This way you get memory efficiency when it matters.&lt;/p&gt;

&lt;h3 id=&#34;sip-hasher&#34;&gt;Sip-hasher?!?&lt;/h3&gt;

&lt;p&gt;Sip-hasher is a cryptographic hash function, and as with most cryptographic hash functions, it is slower than the non-cryptographic counterpart.&lt;/p&gt;

&lt;p&gt;And it doesn&amp;rsquo;t even have a measurable better quality. I tried giving three different hash functions various data sets. Collision-wise they did equally on every data set, with exception of the English dictionary (as seen below). In every single test, my home-made hash function &amp;ldquo;long hasher&amp;rdquo; beats sip-hasher on performance &lt;em&gt;by a significant factor&lt;/em&gt; (around 30%)&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~ SipHasher
    Filled buckets: 2048
    Max bucket: 245
    Time: 0 s. 39232 ms.
    GB/s: 0.8565382742517557
~ DJB2
    Filled buckets: 2048
    Max bucket: 238
    Time: 0 s. 39463 ms.
    GB/s: 0.784737479297558
~ LongHasher
    Filled buckets: 2048
    Max bucket: 239
    Time: 0 s. 29562 ms.
    GB/s: 4.95375004585191
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some investigation shows that the vast majority (98%) of the time of retrieval is used on hashing (it&amp;rsquo;s not clear if the same is true for insertions, but it still have a major influence)). Say you used hash maps for caching database queries. That could potentially translate to 30% faster retrieval on cached queries.&lt;/p&gt;

&lt;p&gt;My point isn&amp;rsquo;t that LongHasher is fantastic, but my point is that there are hash functions which vastly beats sip hasher performance-wise.&lt;/p&gt;

&lt;p&gt;On a side note: These numbers are quite impressive, and if you don&amp;rsquo;t believe me &lt;a href=&#34;https://gist.github.com/anonymous/3b0b489137af9006d5c498f10d42514a&#34;&gt;you can run it yourself&lt;/a&gt;. The reason that long hasher is able to outperform them both is that it consumes eight bytes at once. Otherwise, it is really just multiplying by a prime, adding some constant, multiplying by another byte, rotating right and then XORing by some key.&lt;/p&gt;

&lt;p&gt;Now, if it isn&amp;rsquo;t quality, then what&amp;rsquo;s the reason for using a cryptographic hash function? One reason often cited is Denial-of-Service resistance, and that&amp;rsquo;s a valid concern, but is it really something that everyone should pay for?&lt;/p&gt;

&lt;p&gt;You may know the famous &amp;ldquo;What you don&amp;rsquo;t use, you don&amp;rsquo;t pay for&amp;rdquo; idiom. This is a core part of the &amp;ldquo;abstraction without overhead&amp;rdquo; principle. It is relatively rare to actually need DoS-protection, and you pay for this whenever you use &lt;code&gt;HashMap&lt;/code&gt; without overwriting the hash function.&lt;/p&gt;

&lt;p&gt;And, ignoring that point for a moment, The idea that your code is &amp;lsquo;secure by default&amp;rsquo; is a dangerous one and promotes ignorance about security. You code is &lt;em&gt;not&lt;/em&gt; secure by default.&lt;/p&gt;

&lt;p&gt;If you really do need a fast and yet secure hash function, Sip-hasher is a wonderful choice. To be clear, I&amp;rsquo;m not arguing against Sip-hasher (I actually like the function), but rather against having it as a default choice.&lt;/p&gt;

&lt;h2 id=&#34;btreemap&#34;&gt;&lt;code&gt;BTreeMap&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;BTreeMap&lt;/code&gt; and &lt;code&gt;BTreeSet&lt;/code&gt; are generally a good implementation. My only criticism has to do with cache efficiency, which is notoriously bad. In fact, around 60% of the links followed leads to cache misses. For a map 1000 elements, a lookup would result in approximately 6 cache misses. For 10000, the number is 8.&lt;/p&gt;

&lt;p&gt;These can be quite expensive. A solution is proposed in the section about &amp;ldquo;Cache-efficient structures&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;vecdeque&#34;&gt;&lt;code&gt;VecDeque&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;VecDeque&lt;/code&gt; is a decent implementation. The only problem is that you cannot range index (slice) it. This is due to the very nature of ring buffers.&lt;/p&gt;

&lt;p&gt;An alternative to conventional ring buffers is &lt;a href=&#34;http://www.codeproject.com/Articles/3479/The-Bip-Buffer-The-Circular-Buffer-with-a-Twist&#34;&gt;biparite buffers&lt;/a&gt;, which has essentially the same performance, but allows this (and other interesting) API.&lt;/p&gt;

&lt;h2 id=&#34;mpsc&#34;&gt;MPSC&lt;/h2&gt;

&lt;p&gt;MPSC is a popular tool for message passing, but a critical point is often overlooked: Every queuing/dequeueing involves a malloc call. That sounds pretty bad, doesn&amp;rsquo;t it?&lt;/p&gt;

&lt;p&gt;MPSC is supposed to be lock-less, but that isn&amp;rsquo;t the case if the tcache is empty. Then it involves a lock.&lt;/p&gt;

&lt;p&gt;Since you effectively queue and dequeue all of the time, you actually waste allocations going in and out the allocator. That&amp;rsquo;s a major overhead, and totally unreflected in the API, giving an illusion of zero-cost.&lt;/p&gt;

&lt;p&gt;And, it turns out that it isn&amp;rsquo;t necessary. Because of the ring-buffer-like structure of MPSC, you can effectively store it all in a concurrent SLOB list, making malloc calls incredibly rare (ideally only upon the first queue).&lt;/p&gt;

&lt;p&gt;My benchmarks I&amp;rsquo;ve made on &lt;a href=&#34;https://github.com/redox-os/ralloc&#34;&gt;ralloc&lt;/a&gt;, a memory allocator I wrote, (which uses mpsc internally for cross-thread frees) shows a significant performance gain. An exercise for the reader is to do the same for Servo and try to see if it affects performance.&lt;/p&gt;

&lt;h2 id=&#34;vec&#34;&gt;&lt;code&gt;Vec&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;My criticism of &lt;code&gt;Vec&lt;/code&gt; is the lack of API for manual management. One particular missing thing is the ability to replace the reallocation strategy with a custom one.&lt;/p&gt;

&lt;p&gt;Vectors are used everywhere and they often have different usage patterns, many of which can be exploited to improve performance and memory efficiency.&lt;/p&gt;

&lt;p&gt;Another thing I sometimes need is the ability to get a mutable reference to the element I pushed without needing extra bound checks (note that in most cases this is a trivial optimization for LLVM, but it adds a lot of convenience). This could simply be solved by having &lt;code&gt;push&lt;/code&gt; return &lt;code&gt;&amp;amp;mut T&lt;/code&gt;. This is technically a breaking change but I doubt it will affect anybody.&lt;/p&gt;

&lt;h1 id=&#34;cache-efficient-structures&#34;&gt;Cache-efficient structures&lt;/h1&gt;

&lt;p&gt;I spoke a little about SLOB-based arenas previously. It turns out to have major impact on the cache efficiency, and thereby the performance, of the structure.&lt;/p&gt;

&lt;p&gt;The idea is that each structure holds an arena which only spans a few memory pages, ensure data locality. Obviously, this is more memory hunky, but it is conceptually similar to vectors which reserve extra memory to avoid reallocation. In this case, we are looking for avoiding allocation instead.&lt;/p&gt;

&lt;p&gt;Depending on what you&amp;rsquo;re doing it affects the performance positively by 3-10% (B-trees), 5-15% (linked lists), or 40-80% (mpsc). Those numbers are quite impressive (especially the last one).&lt;/p&gt;

&lt;h1 id=&#34;replacing-the-allocator&#34;&gt;Replacing the allocator&lt;/h1&gt;

&lt;p&gt;Another lacking feature is an &lt;code&gt;Allocator&lt;/code&gt; trait, which is intended to be the bound of some generic parameter in all the collections, allowing you to replace the allocator to exploit allocation patterns of the structure.&lt;/p&gt;

&lt;p&gt;An RFC for exactly this &lt;a href=&#34;https://github.com/rust-lang/rfcs/pull/1398&#34;&gt;already exists&lt;/a&gt; and is merged, but the implementation is incomplete.&lt;/p&gt;

&lt;h1 id=&#34;hiding-box&#34;&gt;Hiding &lt;code&gt;Box&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;An unfortunate thing is hiding the overhead by letting the function itself allocate, instead of letting the caller do it. This is (or at least, should be) considered bad style, because the API ought to reflect the semantics and performance characteristics. If the allocation is hidden to the programmer, she might not realize the expensive operations behind the scenes.&lt;/p&gt;

&lt;h1 id=&#34;the-good-parts&#34;&gt;The good parts&lt;/h1&gt;

&lt;p&gt;The implementations them self are really good and well-tested. Nothing&amp;rsquo;s wrong there. It is more the implementation and API choices, which are &amp;ldquo;horrible&amp;rdquo;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lambda crabs (part 3): Region-based alias analysis</title>
      <link>/blog/lambda_crabs_3/</link>
      <pubDate>Wed, 08 Jun 2016 11:24:24 +0200</pubDate>
      
      <guid>/blog/lambda_crabs_3/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;http://ticki.github.io/blog/lambda_crabs_2/&#34;&gt;last post&lt;/a&gt;, we saw how to
infer regions and their span. In this post, we will cover aliasing and how to
ensure guarantees through region analysis.&lt;/p&gt;

&lt;h2 id=&#34;aliasing-mutable-aliasing-and-unsafety&#34;&gt;Aliasing, mutable aliasing, and unsafety.&lt;/h2&gt;

&lt;p&gt;Two pointers are said to be &lt;em&gt;aliased&lt;/em&gt;, if they refer to the same object. Alias
analysis is essential to program verification, optimizers, and compiler theory.&lt;/p&gt;

&lt;p&gt;Alias analysis is the study of which pointers are aliased and, more
importantly, which pointers &lt;em&gt;aren&amp;rsquo;t aliased&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Rust guarantees that no mutable reference is aliased. This is statically
checked, and we will show how in this post.&lt;/p&gt;

&lt;p&gt;So, why is aliasing guarantees even needed?&lt;/p&gt;

&lt;p&gt;The answer is that I need to be able to reason about the invariants of the
pointers content, while being sure that those aren&amp;rsquo;t broken in the period of
accessibility.&lt;/p&gt;

&lt;p&gt;Furthermore, we want strict thread-safety, which requires guarantees about
shared mutable state.&lt;/p&gt;

&lt;h2 id=&#34;different-values-different-namespaces&#34;&gt;Different values, different namespaces&lt;/h2&gt;

&lt;p&gt;To reason about mutability overlaps and aliasing through regions, we need a
notion of different namespaces.&lt;/p&gt;

&lt;p&gt;For example, say some variable X is referenced in a lifetime &lt;code&gt;&#39;a&lt;/code&gt;. Does that
mean another variable Y living in the same scope can&amp;rsquo;t be mutated?&lt;/p&gt;

&lt;p&gt;Of course not! Let&amp;rsquo;s consider:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;{
    let mut a = 2; // ------+ &#39;a
    let b = &amp;amp;a;    // ----+ | &#39;b
    let mut c = 0; // --+ | | &#39;c
    c = 1;         //   | | |
    c = 2;         //   | | |
} // -------------------+-+-+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see &lt;code&gt;a&lt;/code&gt; is aliased, thus mutating it is not allowed. However, &lt;code&gt;&#39;a:
&#39;c&lt;/code&gt;, but that doesn&amp;rsquo;t mean they refer to the same.&lt;/p&gt;

&lt;p&gt;Holding a global namespace would make the above example fail, since it has no
distinction between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; and their respective lifetimes.&lt;/p&gt;

&lt;p&gt;For that reason, we need to segregate the regions, such that we can effectively
reason about aliasing without mixing values up.&lt;/p&gt;

&lt;h2 id=&#34;sublattices&#34;&gt;Sublattices&lt;/h2&gt;

&lt;p&gt;We talked a bit about lattices and their applications in the last part. I
recommend reading that if you do not know what a lattice is.&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s introduce the notion of a &lt;em&gt;sublattice&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;M&lt;/em&gt; is a sublattice of &lt;em&gt;L&lt;/em&gt;, if &lt;em&gt;M&lt;/em&gt; is a nonempty subset of &lt;em&gt;L&lt;/em&gt; forming a
lattice under &lt;em&gt;L&lt;/em&gt;&amp;rsquo;s meet and join operators.&lt;/p&gt;

&lt;p&gt;Take a lattice,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;       Join(a, b, c)
           /\
          /  \
         /    \
   Join(a, b)  \c
       /\      /
      /  \    /
     /    \  /
   a/      \/b
    \      /
     \    /
      \  /
       \/
    Meet(a, b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(by the way, this is why it is called a lattice)&lt;/p&gt;

&lt;p&gt;then&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   Join(a, b)
       /\
      /  \
     /    \
  a /      \b
    \      /
     \    /
      \  /
       \/
    Meet(a, b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;is a sublattice, since it holds all the conditions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;It is a nonempty subset.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;It shares meet and join, while preserving closure (you can easily check this
yourself).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;region-classes&#34;&gt;Region classes&lt;/h2&gt;

&lt;p&gt;Region classes has many names, but none which is universally agreed upon, I
prefer the name region classes. As we say, a rose by any other name would still
smell as sweet.&lt;/p&gt;

&lt;p&gt;Let &lt;em&gt;L&lt;/em&gt; be our region lattice, define a &lt;em&gt;region class&lt;/em&gt; of &lt;em&gt;L&lt;/em&gt; as a sublattice
of &lt;em&gt;L&lt;/em&gt;, in the context of segregating regions.&lt;/p&gt;

&lt;p&gt;In particular, assign each value a region class. Say the value has the bounds
(outlives) &lt;code&gt;{a, b, c, d...}&lt;/code&gt;, then we derive our region class as the cyclic
sublattice, &lt;code&gt;&amp;lt;a, b, c, d...&amp;gt;&lt;/code&gt;. In particular, this means &lt;em&gt;the smallest
extension which forms a sublattice of L&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The compiler keeps a log of the region class of every variable. This is then
used for alias analysis:&lt;/p&gt;

&lt;h2 id=&#34;pointers-and-references&#34;&gt;Pointers and references&lt;/h2&gt;

&lt;p&gt;Taking an immutable reference extends our region class to contain the region of
this particular reference, denoted &lt;code&gt;M[N]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mutable references, on the other hand, works slightly different. The region
class and the region of the reference &lt;em&gt;must be disjoint&lt;/em&gt;, unless we get shared
mutability. With this requirement satisfied, we can proceed to extend the
region class with the new region.&lt;/p&gt;

&lt;h2 id=&#34;mutating-a-local-variable&#34;&gt;Mutating a local variable&lt;/h2&gt;

&lt;p&gt;You may ask, &amp;ldquo;Can you mutate a local variable while it is borrowed?&amp;rdquo;, the
answer is, &amp;ldquo;No, you cannot&amp;rdquo;. The reason is the same for the mutable aliasing:
it introduce shared mutable state.&lt;/p&gt;

&lt;p&gt;But, how do we handle such mutations?&lt;/p&gt;

&lt;p&gt;We introduced &lt;code&gt;empty(x)&lt;/code&gt;, the empty region at &lt;code&gt;x&lt;/code&gt;, in the earlier blog posts.
And we can use this to interpret local mutations as well: as taking a mutable
reference for region &lt;code&gt;empty(x)&lt;/code&gt; and simply mutate it through the reference.&lt;/p&gt;

&lt;h2 id=&#34;applying-this-method&#34;&gt;Applying this method&lt;/h2&gt;

&lt;p&gt;If we get back to our example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;{
    let mut a = 2; // ------+ &#39;a
    let b = &amp;amp;a;    // ----+ | &#39;b
    let mut c = 0; // --+ | | &#39;c
    c = 1;         //   | | |
    c = 2;         //   | | |
} // -------------------+-+-+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see that the region class of &lt;code&gt;&#39;a&lt;/code&gt; is an extension of &lt;code&gt;&#39;b&lt;/code&gt;, but &lt;code&gt;&#39;c&lt;/code&gt; is
not entangled with &lt;code&gt;&#39;a&lt;/code&gt;&amp;rsquo;s region class. In particular, &lt;code&gt;&#39;c&lt;/code&gt; and &lt;code&gt;&#39;a&lt;/code&gt; belong to
different namespaces and thus, there is no shared mutability.&lt;/p&gt;

&lt;h2 id=&#34;region-classes-and-their-relations&#34;&gt;Region classes and their relations&lt;/h2&gt;

&lt;p&gt;A natural question that arise is: Why don&amp;rsquo;t we do region inference seperately
for each region class?&lt;/p&gt;

&lt;p&gt;The answer is that distinct region classes are far from unrelated. Each region
class simply defines a value and its aliases, but that doesn&amp;rsquo;t make it isolated
for the rest of &lt;em&gt;L&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If you look at our example above, you may notice that &lt;code&gt;&#39;a&lt;/code&gt; outlives &lt;code&gt;&#39;c&lt;/code&gt;,
despite being associated with a different region class.&lt;/p&gt;

&lt;h2 id=&#34;questions-and-errata&#34;&gt;Questions and errata&lt;/h2&gt;

&lt;p&gt;Ping me at #rust in Mozilla IRC.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lambda crabs (part 2): Region inference is (not) magic.</title>
      <link>/blog/lambda_crabs_2/</link>
      <pubDate>Mon, 06 Jun 2016 11:06:21 +0200</pubDate>
      
      <guid>/blog/lambda_crabs_2/</guid>
      <description>

&lt;p&gt;This post will cover region (lifetime) inference with a mathematical and type
theoretical focus.&lt;/p&gt;

&lt;h2 id=&#34;the-problem&#34;&gt;The problem&lt;/h2&gt;

&lt;p&gt;Inference is a very handy concept. We no longer have to annotate redundant
types, which is a major pain point in languages, that lacks of type inference.&lt;/p&gt;

&lt;p&gt;Now, we want such an inference scheme for regions as well.&lt;/p&gt;

&lt;p&gt;We described the problem of region inference in &lt;a href=&#34;/blog/lambda_crabs_1/&#34;&gt;last post&lt;/a&gt; as:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;So, this is just a classical optimization problem:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;  minimize    &#39;a
  subject to  A, B, C...

  A, B, C… are outlives relations. ‘a may or may not be free in those.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Namely, we want to minimize some lifetimes, while holding some conditions.&lt;/p&gt;

&lt;h2 id=&#34;adding-regions&#34;&gt;&amp;ldquo;Adding&amp;rdquo; regions&lt;/h2&gt;

&lt;p&gt;One thing we will use throughout the region inference algorithm is the notion of &amp;ldquo;adding&amp;rdquo; regions.&lt;/p&gt;

&lt;p&gt;You may have seen &lt;code&gt;&#39;a + &#39;b&lt;/code&gt; before. Intuitively, &lt;code&gt;&#39;a: &#39;b + &#39;c&lt;/code&gt; is equivalent to
&lt;code&gt;&#39;a: &#39;b, &#39;a: &#39;c&lt;/code&gt;, but we can go further and use &lt;code&gt;&#39;a + &#39;b&lt;/code&gt; as a way to construct
new regions:&lt;/p&gt;

&lt;p&gt;Define &lt;code&gt;&#39;a + &#39;b&lt;/code&gt; as the smallest region that outlives both &lt;code&gt;&#39;a&lt;/code&gt; and &lt;code&gt;&#39;b&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In a sense, you &amp;ldquo;widen&amp;rdquo; the region until it covers both regions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a:       I---------I
&#39;b:            I------------I
&#39;a + &#39;b:  I-----------------I
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;funky-but-useless-regions-under-addition-as-an-abelian-semigroup&#34;&gt;Funky but useless: Regions under addition as an abelian semigroup&lt;/h2&gt;

&lt;p&gt;A semigroup is an algebraic structure satisfying two properties:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Closure, for any a, b in S, a + b is contained in S.&lt;/li&gt;
&lt;li&gt;Associativity, for any a, b, and c in S, (a + b) + c = a + (b + c).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But in contrary to monoids, there is no identity element.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Abelian&amp;rdquo; means commutative. That is, a + b = b + a.&lt;/p&gt;

&lt;p&gt;And, in fact, regions follows all these rules, making it an abelian semigroup.&lt;/p&gt;

&lt;p&gt;We know two additional facts about our operator:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It follows from the fact &lt;code&gt;&#39;a: &#39;a&lt;/code&gt;, that a + a = a&lt;/li&gt;
&lt;li&gt;It follows from the fact &lt;code&gt;&#39;static: &#39;a&lt;/code&gt; for all &lt;code&gt;&#39;a&lt;/code&gt;, that &lt;code&gt;∃s∈L  ∀a∈L  s + a = s&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;regions-as-a-lattice&#34;&gt;Regions as a lattice&lt;/h2&gt;

&lt;p&gt;It makes much more sense to think of regions as a lattice. A lattice is a poset
with two operators defined on it:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Join, an unique supremum (that is, least upper-bound). This is our &lt;code&gt;+&lt;/code&gt;
operator.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Meet, an unique infimum (that is, greatest lower-bound). This isn&amp;rsquo;t very
useful for the matter of regions, but it is still defined on them.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;which follows a set of laws:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The law of commutativity: Both meet and join are commutative operators.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The law of associativity: Both meet and join are associative operators.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The law of absorption: Meet(a, Meet(a, b)) = Meet(a, b) and Join(a, Join(a, b)) = Join(a, b).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In fact, this describes our structure perfectly. In particular, L is an
&lt;em&gt;upper-bounded lattice&lt;/em&gt;, i.e. we have a maximal element (&lt;code&gt;&#39;static&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Lattice theory, which we will cover in-depth in a later post is perfect for
studying subtyping relations.&lt;/p&gt;

&lt;h2 id=&#34;directed-acyclic-graphs&#34;&gt;Directed Acyclic Graphs&lt;/h2&gt;

&lt;p&gt;A directed acyclic graph is a finite directed graph with no directed cycles.
That is, any arbitrary directed walk in the graph will &amp;ldquo;end&amp;rdquo; at some point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/61/Polytree.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s forget the &lt;code&gt;&#39;a: &#39;a&lt;/code&gt; case for a moment. As such, the regions under our
&lt;em&gt;strict&lt;/em&gt; outlive relation, &lt;em&gt;&amp;lt;&lt;/em&gt;, forms a directed acyclic graph (DAG).&lt;/p&gt;

&lt;p&gt;In particular, if two node are connected, with a directed edge A → B, A
represents a region, which &lt;em&gt;outlives&lt;/em&gt; B.&lt;/p&gt;

&lt;p&gt;Consider we take a reference &lt;code&gt;&amp;amp;&#39;b T&lt;/code&gt; where &lt;code&gt;T: &#39;a&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; &#39;static
 |
 v
&#39;a &amp;lt;---------\
 |           |
 |           |
 |           |
 v           |
&#39;b &amp;lt;------- &#39;a + &#39;b
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;handling-cycles&#34;&gt;Handling cycles&lt;/h2&gt;

&lt;p&gt;Every lifetime outlives itself, as explained in the last post. So our outlives
relation doesn&amp;rsquo;t form a DAG, due to these cycles.&lt;/p&gt;

&lt;p&gt;The solution is relatively simple, though.&lt;/p&gt;

&lt;p&gt;Let &lt;code&gt;{&#39;a, &#39;b, &#39;c, ...}&lt;/code&gt; be cycle such that &lt;code&gt;&#39;a &amp;lt; &#39;b &amp;lt; &#39;c ... &amp;lt; &#39;a&lt;/code&gt;. Due to
transitivity and antisymmetry, we can assume that &lt;code&gt;&#39;a = &#39;b = &#39;c = ...&lt;/code&gt;, thus we
can, without loss of generality, collapse the cycle into a single node.&lt;/p&gt;

&lt;p&gt;This lets us interpret the graph, where edges represents outlives relations, as
a DAG.&lt;/p&gt;

&lt;h2 id=&#34;recursively-widening-the-regions&#34;&gt;Recursively widening the regions&lt;/h2&gt;

&lt;p&gt;Say we want to infer the span of some node &lt;code&gt;&#39;a&lt;/code&gt;. Assume &lt;code&gt;&#39;a&lt;/code&gt; neighbors (outlives)
&lt;code&gt;&#39;b, &#39;c, &#39;d...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Since we know the bound, we can say &lt;code&gt;&#39;a = &#39;b + &#39;c + &#39;d + ...&lt;/code&gt;, since this is the
smallest &amp;lsquo;a subject to the outlives conditions.&lt;/p&gt;

&lt;p&gt;Now, recursively do the same with &lt;code&gt;&#39;b, &#39;c, &#39;d, ...&lt;/code&gt; Since the graph is acyclic,
this will terminate at some point.&lt;/p&gt;

&lt;p&gt;On an implementation note: you can optimize this process by 1. deduplicating
the regions, 2. collapsing sums containing &lt;code&gt;&#39;static&lt;/code&gt; into &lt;code&gt;&#39;static&lt;/code&gt;, 3. caching the
nodes to avoid redundant calculations.&lt;/p&gt;

&lt;h2 id=&#34;going-further-liveness&#34;&gt;Going further: liveness&lt;/h2&gt;

&lt;p&gt;Now that we have a closed form for inferring lifetimes, we can do lots of cool stuff.&lt;/p&gt;

&lt;p&gt;Liveness of a value is the span starting where the value is declared and ending
where the last access to it is made. This is in contrary to the classical
lexical approach, where the initial lifetimes are assigned as the scopes of the
variables.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start by defining &lt;code&gt;empty(x)&lt;/code&gt; as the region spanning from x to x (that is,
an empty region at x). Assign every value declared at x a region, &lt;code&gt;empty(x)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Whenever a value of lifetime &lt;code&gt;&#39;x&lt;/code&gt; is used at some point y, we add a bound &lt;code&gt;&#39;x:
empty(y)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So we essentially expand the region whenever used, effectively yielding the
liveness of the value.&lt;/p&gt;

&lt;h2 id=&#34;a-happy-ending&#34;&gt;A happy ending&lt;/h2&gt;

&lt;p&gt;That&amp;rsquo;s it&amp;hellip; The algorithm is really that simple. In fact, you can implement it
in only a 100-200 lines.&lt;/p&gt;

&lt;h2 id=&#34;questions-and-errata&#34;&gt;Questions and errata&lt;/h2&gt;

&lt;p&gt;Ping me at #rust in Mozilla IRC.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lambda crabs (part 1): A mathematical introduction to lifetimes and regions</title>
      <link>/blog/lambda_crabs_1/</link>
      <pubDate>Mon, 06 Jun 2016 09:12:56 +0200</pubDate>
      
      <guid>/blog/lambda_crabs_1/</guid>
      <description>

&lt;p&gt;This post will cover lifetimes and regions in depth, with a focus on the mathematical background of regions. That is, what is a region? What rules do they follow? How does the compiler handle them? And how are they inferred?&lt;/p&gt;

&lt;h2 id=&#34;regions-and-their-ordering&#34;&gt;Regions and their ordering&lt;/h2&gt;

&lt;p&gt;So, let&amp;rsquo;s briefly investigate what a region is. A region (or in Rust lingo, a lifetime) is a span of some form, e.g. the token stream. Regions have an outlive relation defined on them.&lt;/p&gt;

&lt;p&gt;A region &lt;code&gt;&#39;a&lt;/code&gt; outlives &lt;code&gt;&#39;b&lt;/code&gt; if &lt;code&gt;&#39;b&lt;/code&gt;&amp;rsquo;s span is covered by &lt;code&gt;&#39;a&lt;/code&gt;. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a: I----------------I
&#39;b: I---------I
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see &lt;code&gt;&#39;a: &#39;b&lt;/code&gt; since the first span covers the second. But what is the nature of the outlives relation?&lt;/p&gt;

&lt;h2 id=&#34;regions-a-poset&#34;&gt;Regions: a poset&lt;/h2&gt;

&lt;p&gt;One could mistakenly believe that regions are ordered over their outlives relation. An totally ordered set A under ≤ means that any elements a, b ∈ A satisfy all of the following statements:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;If a ≤ b and b ≤ a are both satisfied, a = b.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If a ≤ b and b ≤ c are both satisfied, a ≤ c.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;At least one of a ≤ b and b ≤ a is true.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To see why the outlives relation is not a total order over the set of regions, consider the case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a: I---------I
&#39;b:    I------------I
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The third condition is not met here: neither &lt;code&gt;&#39;a: &#39;b&lt;/code&gt; or &lt;code&gt;&#39;b: &#39;a&lt;/code&gt; is true.&lt;/p&gt;

&lt;p&gt;It turns out that weakening the last condition to only consider reflexivity gives us a structure, that L (the set of regions) classifies. Replace 3. by a ≤ a, and you get a partially ordered set, or a poset.&lt;/p&gt;

&lt;h2 id=&#34;outlive-relation-as-a-partial-order&#34;&gt;Outlive relation as a partial order&lt;/h2&gt;

&lt;p&gt;So, let&amp;rsquo;s briefly explain how the rules of outliving mirrors the rules of partial orders.&lt;/p&gt;

&lt;p&gt;The first rule, the rule of antisymmetry, reads&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a: &#39;b
&#39;b: &#39;a
-------
&#39;a = &#39;b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So if two regions (lifetimes, borrows, scopes, etc.) outlives each other symmetrically (&amp;lsquo;a: &amp;lsquo;b and &amp;lsquo;b: &amp;lsquo;a), they are, in fact, the same.&lt;/p&gt;

&lt;p&gt;The second rule, the rule of transitivity, is crucial to understanding the semantics of regions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a: &#39;b
&#39;b: &#39;c
------
&#39;a: &#39;c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In other words, regions are hierarchical. It might seem very simple, but the implications are in fact very important: it allows us to conclude things from transitivity. Think of it like you can &amp;ldquo;inherit&amp;rdquo; bounds from outliving regions.&lt;/p&gt;

&lt;p&gt;For example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a: I--------------------I
&#39;b:   I----------------I
&#39;c:      I---------I
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Say we know that, &lt;code&gt;&#39;a: &#39;b&lt;/code&gt;, and &lt;code&gt;&#39;b: &#39;c&lt;/code&gt;. We can then conclude that &lt;code&gt;&#39;a: &#39;c&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The last rule simply states that &amp;lsquo;a outlives itself. This might seem counterintuitive due to the odd terminology, but think of outlives as &amp;ldquo;outlives or equals to&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;In fact, there is only one more thing we know about regions: they have an unique maximal extrema, which outlives all other regions, &lt;code&gt;&#39;static&lt;/code&gt;. Namely, &lt;code&gt;&#39;static&lt;/code&gt; outlives any region, &lt;code&gt;&#39;a&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;And that&amp;rsquo;s all the &amp;ldquo;axioms&amp;rdquo; of lifetimes.&lt;/p&gt;

&lt;h2 id=&#34;what-subtyping-is&#34;&gt;What subtyping is&lt;/h2&gt;

&lt;p&gt;Before we go to next section, we will just have to briefly cover subtyping. τ is said to be a subtype of υ (denoted &lt;code&gt;τ &amp;lt;: υ&lt;/code&gt;), if a &lt;em&gt;type mismatch&lt;/em&gt;, such that τ is inferred to be of type υ, makes the value of type τ coerce into a value of type υ.&lt;/p&gt;

&lt;p&gt;In other words, you can replace your subtype by a supertype (the parent type) without getting a type mismatch error.&lt;/p&gt;

&lt;h2 id=&#34;regions-are-just-types-outlive-relation-as-a-subtyping-rule&#34;&gt;Regions are just types: Outlive relation as a subtyping rule&lt;/h2&gt;

&lt;p&gt;If you think about it, you may notice that lifetimes are used in type positions &lt;em&gt;a lot&lt;/em&gt;. This is no coincidence, since &lt;em&gt;regions are just types with a subtyping relation&lt;/em&gt;, which is the very reason you are allowed to do e.g. &lt;code&gt;MyStruct&amp;lt;&#39;a&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In fact, the outlive relation defines a subtyping rule. That is, you can always &amp;ldquo;shrink&amp;rdquo; a region span. Let &lt;em&gt;c&lt;/em&gt; be a type constructor, &amp;lsquo;a → *, then &amp;lsquo;a: &amp;lsquo;b implies that &lt;code&gt;&#39;a &amp;lt;: &#39;b&lt;/code&gt;, that is &lt;code&gt;&#39;a&lt;/code&gt; can coerce into &lt;code&gt;&#39;b&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For example, &lt;code&gt;&amp;amp;&#39;static str&lt;/code&gt; can coerce to any &lt;code&gt;&amp;amp;&#39;a str&lt;/code&gt;, since &lt;code&gt;&#39;static&lt;/code&gt; outlives any lifetime.&lt;/p&gt;

&lt;p&gt;Due to the implementation, there are a few limits, though. You can for example not do &lt;code&gt;let a: &#39;a&lt;/code&gt; which would be useless anyways.&lt;/p&gt;

&lt;p&gt;Syntactically, there is a confusion: lifetimes appears in certain trait places, especially in &lt;em&gt;trait bounds&lt;/em&gt;. But, in fact, that is only a syntactic sugar for an imaginary trait, let&amp;rsquo;s call it &lt;code&gt;Scope&lt;/code&gt;, which takes a lifetime.&lt;/p&gt;

&lt;p&gt;This represents the scope of a type, so when writing &lt;code&gt;fn my_func::&amp;lt;T: &#39;static&amp;gt;()&lt;/code&gt; you can think of it as writing &lt;code&gt;fn my_func::&amp;lt;T: Scope&amp;lt;&#39;static&amp;gt;&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Due to the coercion rules (which will be covered in a future post), this means that if &lt;code&gt;T: Scope&amp;lt;&#39;a&amp;gt;&lt;/code&gt; and &lt;code&gt;U: Scope&amp;lt;&#39;b&amp;gt;&lt;/code&gt; with &lt;code&gt;&#39;a: &#39;b&lt;/code&gt;, then &lt;code&gt;T&lt;/code&gt; is a subtype of &lt;code&gt;U&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;inferring-regions&#34;&gt;Inferring regions.&lt;/h2&gt;

&lt;p&gt;This is the exciting part. Rust has region inference, allowing it to infer the lifetimes in your program.&lt;/p&gt;

&lt;p&gt;Due to Rust&amp;rsquo;s aliasing guarantees, it tries to &lt;em&gt;minimize&lt;/em&gt; the region&amp;rsquo;s span, while still satisfying the conditions (outlives relations) given.&lt;/p&gt;

&lt;p&gt;So, this is just a classical optimization problem:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minimize    &#39;a
subject to  A, B, C...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A, B, C&amp;hellip; are outlives relations. &lt;code&gt;&#39;a&lt;/code&gt; may or may not be free in those.&lt;/p&gt;

&lt;p&gt;We will cover how we actually solve this optimization problem in a future blog post, but until then you can see if you can find an algorithm to do so ;).&lt;/p&gt;

&lt;h2 id=&#34;questions-and-errata&#34;&gt;Questions and errata&lt;/h2&gt;

&lt;p&gt;Ping me at #rust in Mozilla IRC.&lt;/p&gt;

&lt;h2 id=&#34;credits&#34;&gt;Credits&lt;/h2&gt;

&lt;p&gt;Credits to Yaniel on IRC for the idea for the name of this series. It is based on the famous &amp;ldquo;lambda cats&amp;rdquo; series, but since Ferris, the crab, is our Rust mascot, we do lambda crabs, instead.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>